{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ec598f-e0d9-44af-9617-6b5f16fa7f65",
   "metadata": {},
   "source": [
    "# Upsampling\n",
    "\n",
    "Everyone makes mistakes, and TTS does a lot of them. \n",
    "\n",
    "The intuition is you need to remove all those from the training set. I didn't check it experimentally with the current setup, but from prior attempts I know that cutting sentences short renders VITS training unable to read some tailing dull sounds like \"g\" or \"b\" at all, and I had to build workarounds for Tortoise to make sure it reads them. Piper also uses VITS for training so probably it's still the case. However, I didn't research to what extent problems of these kind affect the training.\n",
    "\n",
    "So to remove these and other errors, I implemented several steps from `brainbox.flow', which I will explain in this demo.\n",
    "\n",
    "First, let's install the required deciders. Instead of Zonos, we will CoquiTTS, because it's a bit faster and also less demanding to the resources and the CUDA drivers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7635d99f-b611-43ac-9b8f-8756869f6f38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T08:16:11.226824Z",
     "iopub.status.busy": "2025-05-01T08:16:11.226732Z",
     "iopub.status.idle": "2025-05-01T08:16:13.765344Z",
     "shell.execute_reply": "2025-05-01T08:16:13.764058Z"
    }
   },
   "outputs": [],
   "source": [
    "from brainbox import BrainBox\n",
    "from brainbox.deciders import CoquiTTS, Vosk, Resemblyzer\n",
    "\n",
    "api = BrainBox.Api('127.0.0.1:8090')\n",
    "\n",
    "for decider in [CoquiTTS, Vosk, Resemblyzer]:\n",
    "    api.controller_api.install_if_not_installed(decider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d6e31-5394-44ac-bf65-7fe8815cc2b7",
   "metadata": {},
   "source": [
    "Now, let's define the steps. The steps will refine an array of `UpsamplingItem` objects by filling its fields. We will start with the items that represent a text dataset we upsample. To each sentence, I will add prefix and suffix: Zonos and TortoiseTTS have a tendency to fail exactly at the end and at the beginning of the sentences, so this trick will help to improve the successfulness of the upsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4839708b-8103-4c51-acd0-97d3bae5394b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T08:16:13.770495Z",
     "iopub.status.busy": "2025-05-01T08:16:13.769807Z",
     "iopub.status.idle": "2025-05-01T08:16:13.860153Z",
     "shell.execute_reply": "2025-05-01T08:16:13.859852Z"
    }
   },
   "outputs": [],
   "source": [
    "from yo_fluq import *\n",
    "from brainbox.flow import ConstantStep\n",
    "from chara.voice_clone.upsampling import UpsamplingItem\n",
    "\n",
    "sentences = FileIO.read_json('../files/sentences.json')[:10]\n",
    "initial_step = ConstantStep([UpsamplingItem('The beginning. '+sentence+' The end.') for sentence in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749372e8-7176-4866-81f2-cb8a3bf169b5",
   "metadata": {},
   "source": [
    "Then, each text would need a `VoiceCloner` to convert it to sound. This is represented in the following step. This step also allows you to train the voice cloner on the data supplied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7cb86a1-330c-4822-a623-892bd2599593",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T08:16:13.861879Z",
     "iopub.status.busy": "2025-05-01T08:16:13.861722Z",
     "iopub.status.idle": "2025-05-01T08:16:14.775821Z",
     "shell.execute_reply": "2025-05-01T08:16:14.775215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'error': None,\n",
       "  'id': 'id_f15286dcadc043bf9afad578df5a0821',\n",
       "  'result': 'OK',\n",
       "  'tags': {'model_name': 'voice_clone_demo_Axe',\n",
       "   'upsampler': 'CoquiVoiceCloner'}},\n",
       " {'error': None,\n",
       "  'id': 'id_0aab3a97bf7c47fda48fce0ae947f9a7',\n",
       "  'result': 'OK',\n",
       "  'tags': {'model_name': 'voice_clone_demo_Lina',\n",
       "   'upsampler': 'CoquiVoiceCloner'}}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chara.voice_clone import upsampling \n",
    "from chara.voice_clone.voice_cloner import Character, CoquiVoiceCloner\n",
    "from chara.tools import Language\n",
    "from pathlib import Path\n",
    "\n",
    "folder = Path('../files').absolute()\n",
    "characters = [\n",
    "    Character('Axe', folder/'axe'),\n",
    "    Character('Lina', folder/'lina')\n",
    "]\n",
    "cloners = [CoquiVoiceCloner(c, Language.English()) for c in characters]\n",
    "for c in cloners:\n",
    "    c.model_prefix = 'voice_clone_demo'\n",
    "\n",
    "assign_voice_cloners_step =  upsampling.AssignVoiceClonersStep(cloners)\n",
    "assign_voice_cloners_step.get_training_command().with_cache('temp/voice_training.json').execute(api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaef235-3f85-454a-97ea-597d83858aa7",
   "metadata": {},
   "source": [
    "After these two steps, we will have a Cartesian product of all the texts and all the cloners. It is important to exclude the sentences that were already voiced by the characters. One of my experiences was that if you don't do this, if you leave several versions of voiceovers for the same character and same text, the model won't train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cceaa679-2627-4256-9aab-e3d9bb21f658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T08:16:14.779400Z",
     "iopub.status.busy": "2025-05-01T08:16:14.779097Z",
     "iopub.status.idle": "2025-05-01T08:16:14.782122Z",
     "shell.execute_reply": "2025-05-01T08:16:14.781542Z"
    }
   },
   "outputs": [],
   "source": [
    "filter_step = upsampling.FilterStep()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462387a2-e419-48b3-af2b-229787d2b93f",
   "metadata": {},
   "source": [
    "I recommend to further decrease the amount of samples for one voiceover. This way each iteration will take shorter time, and you can review the results in-between. The following step will reduce it to the batch, also favoring the voice cloners that do not yet have enough of successfully processed entries. In this notebook, we define the batch size to 5. In the real environment, set it to 100 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ab5c12f-7dd6-4ef6-85ea-bfdc8e28386a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T08:16:14.785992Z",
     "iopub.status.busy": "2025-05-01T08:16:14.785743Z",
     "iopub.status.idle": "2025-05-01T08:16:14.789073Z",
     "shell.execute_reply": "2025-05-01T08:16:14.788642Z"
    }
   },
   "outputs": [],
   "source": [
    "from brainbox.flow import ProbabilisticEqualRepresentationStep\n",
    "\n",
    "batching_step = ProbabilisticEqualRepresentationStep(10, lambda z: z.voice_cloner.get_voice_cloner_key(), lambda z: z.selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be111ba1-8573-4118-8ef3-0dcdb9584af9",
   "metadata": {},
   "source": [
    "Now we can define the main parts of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a8b5f48-e556-49a8-92dc-37bc66ee28e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T08:16:14.791857Z",
     "iopub.status.busy": "2025-05-01T08:16:14.791671Z",
     "iopub.status.idle": "2025-05-01T08:16:14.794118Z",
     "shell.execute_reply": "2025-05-01T08:16:14.793641Z"
    }
   },
   "outputs": [],
   "source": [
    "voiceover_step_factory = upsampling.VoiceoverStepFactory(api)\n",
    "vosk_step_factory = upsampling.VoskStepFactory(api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be1169c-a368-407d-aa6c-3800aa17c5a7",
   "metadata": {},
   "source": [
    "Additionally, you may want to add Resemblyzer to the system. This will help to determine how close each voiceover is to the original voices. This step may be important for TortoiseTTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d568956e-1a2b-4fd4-848f-7a1555639024",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T08:16:14.797553Z",
     "iopub.status.busy": "2025-05-01T08:16:14.797263Z",
     "iopub.status.idle": "2025-05-01T08:16:21.426798Z",
     "shell.execute_reply": "2025-05-01T08:16:21.426538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': None, 'stats': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resemblyzer_step_factory = upsampling.ResemblyzerStepFactory(api, 'voice_clone_demo')\n",
    "resemblyzer_step_factory.get_training_command([c.samples_folder for c in characters]).with_cache('temp/resemblyzer_training.json').execute(api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a42a3af-1d43-4aaa-aa6f-fec3bc731817",
   "metadata": {},
   "source": [
    "Finally, we need to select the successful voiceovers. This is done with this step. What it does is computing the Levenschtein distance between standardized text and vosk recognition. I will set the max allowed distance to 0, which is very strict, but acceptable for English. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc14cbc-869e-4ece-80dd-beb2a1351cf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T08:16:21.428303Z",
     "iopub.status.busy": "2025-05-01T08:16:21.428069Z",
     "iopub.status.idle": "2025-05-01T08:16:21.429836Z",
     "shell.execute_reply": "2025-05-01T08:16:21.429632Z"
    }
   },
   "outputs": [],
   "source": [
    "selection_step = upsampling.VoskStatisticsStep(upsampling.StringDistance(), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da61c87-44e4-4bb9-b15c-143f0416ab5f",
   "metadata": {},
   "source": [
    "That's it. Now we can run the whole thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "def126bb-b13f-4858-9d0a-089e6f5b517b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T08:16:21.431005Z",
     "iopub.status.busy": "2025-05-01T08:16:21.430849Z",
     "iopub.status.idle": "2025-05-01T08:18:14.959893Z",
     "shell.execute_reply": "2025-05-01T08:18:14.958818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL 0\n",
      "0 records -> Start ConstantStep, 0/8 -> 10 records\n",
      "10 records -> Start AssignVoiceClonersStep, 1/8 -> 20 records\n",
      "20 records -> Start FilterStep, 2/8 -> 20 records\n",
      "20 records -> Start ProbabilisticEqualRepresentationStep, 3/8 -> 10 records (Axe: 6, Lina: 4)\n",
      "10 records -> Start voiceover, 4/8 -> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 records\n",
      "10 records -> Start vosk, 5/8 -> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 records\n",
      "10 records -> Start resemblyzer, 6/8 -> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 records\n",
      "10 records -> Start VoskStatisticsStep, 7/8 -> 10 records (selected 5/10, 50% success rate)\n",
      "TOTAL 10\n"
     ]
    }
   ],
   "source": [
    "from brainbox.flow import Flow\n",
    "\n",
    "flow = Flow(\n",
    "    'temp/upsampling',\n",
    "    [\n",
    "        initial_step,\n",
    "        assign_voice_cloners_step,\n",
    "        filter_step,\n",
    "        batching_step,\n",
    "        voiceover_step_factory.create_step().with_name('voiceover'),\n",
    "        vosk_step_factory.create_step().with_name('vosk'),\n",
    "        resemblyzer_step_factory.create_step().with_name('resemblyzer'),\n",
    "        selection_step\n",
    "    ])\n",
    "flow.reset()\n",
    "flow.run(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c186d52b-e2e2-47a6-84a4-d1dd836adfe9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T08:18:14.965739Z",
     "iopub.status.busy": "2025-05-01T08:18:14.964577Z",
     "iopub.status.idle": "2025-05-01T08:18:14.984067Z",
     "shell.execute_reply": "2025-05-01T08:18:14.983417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "character\n",
       "Axe     0.166667\n",
       "Lina    1.000000\n",
       "Name: selected, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = flow.read_flatten()\n",
    "df = UpsamplingItem.to_df(data)\n",
    "df.groupby('character').selected.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0128ed5-563c-4376-b9e4-e42328e6feff",
   "metadata": {},
   "source": [
    "The following code create an upsampling example for the next step demonstration. Uncomment if you want to update the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c55e2b2-b0eb-4cde-82be-578807f5e0d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T08:18:14.986670Z",
     "iopub.status.busy": "2025-05-01T08:18:14.986355Z",
     "iopub.status.idle": "2025-05-01T08:18:14.992300Z",
     "shell.execute_reply": "2025-05-01T08:18:14.991137Z"
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    df = df.loc[df.selected].feed(fluq.add_ordering_column('character','duration'))\n",
    "    df = df.loc[df.order<2]\n",
    "    for item in df.file:\n",
    "        api.download(item, f'../files/upsampling_example/wavs/{item}')\n",
    "    data = [d for d in flow.read_flatten() if d.voiceover_file in set(df.file)]\n",
    "    FileIO.write_pickle(data, '../files/upsampling_example/records.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6229e3a4-3bc9-439f-8030-9d1a847dec1f",
   "metadata": {},
   "source": [
    "# Additional notes\n",
    "\n",
    "For Zonos, following types of the mistakes are common:\n",
    "1. Reading random rubbish instead of the given text.\n",
    "2. Adding coughing, yawning and all sorts of noise in the beginning.\n",
    "3. Cutting sentence short of 1-2 syllables.\n",
    "\n",
    "These problems are solved with the Vosk recognition and addition of prefixes and suffixes to the texts. Fortunately, Zonos almost always picks the voice of the character correctly, so Resemblyzer step is effectively useless. \n",
    "\n",
    "However, by my experience, TortoiseTTS still picks the intonations and the characters of the voice much better that Zonos. When I train on TortoiseTTS upsampling, the resulting voices are much more vivid. TortoiseTTS, however, often fails to pick the voice correctly, this is why Resemblyzer may still be useful. Also, with TortoiseTTS, the manual annotation of the results is needed, which is also going to be covererd in the next session.\n",
    "\n",
    "## Transferring voice through language\n",
    "\n",
    "Sometimes we want to train e.g. German voice with the samples of English voice. This can be done! However, if English-trained Zonos is producing German voiceovers, the following mistakes will be quite common:\n",
    "\n",
    "* The absence of glottal stops. This makes German completely incomprehensible. \n",
    "* The English pronounciation of words like \"er\" or \"dir\", and generally English \"r\" everywhere\n",
    "\n",
    "\n",
    "If you train the model on this output, the result is going to be horrible. I think this is going to be true for other combinations of languages, not only for English->German.\n",
    "\n",
    "To avoid this, first use the English-trained Zonos to produce ~10 minutes of German voiceovers. It's going to take a long time, as the success rate will be quite low. Then, annotate these voiceovers and pick those that sound German-enough for you. This doesn't have to be absolutely perfect, just better than average.\n",
    "\n",
    "Then, use these annotated voiceovers as samples and produce the final German upsampling. The success rate is going to increase, and the output is going to be much better-sounding. Using Zonos-generated voiceovers as the training data for Zonos decreases the quality, but not to the point of voices to become unrecognizable. \n",
    "\n",
    "## Vosk parameters with German language\n",
    "\n",
    "I recommend setting the transformer for StringDistance that replaces `ß` with `ss`: these two sound exactly the same and Vosk won't be able to pick the correct pronounciation, so 0-distance will be simply impossible.\n",
    "\n",
    "Additionally, German has declinations, and e.g. words like \"jede\" and \"jeder\" are sometimes impossible to distinguish by ear, especially if spoken briefly. So I'd recommend to increase the max allowed distance to 2 or 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
