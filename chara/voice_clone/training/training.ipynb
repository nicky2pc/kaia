{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67337fc8-493c-4185-ab44-91013e282986",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This is the final demonstrational notebook, that post-processes the upsampled records, and builds the training dataset. This dataset will then be fed to the PiperTraining, and the output of the PiperTraining will be evaluated.\n",
    "\n",
    "First, I will upload the demo-files to the brainbox to guarantee reproducibility of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c48752-4a03-45a9-922d-0c9bae0dff35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T08:16:07.494366Z",
     "iopub.status.busy": "2025-05-01T08:16:07.494014Z",
     "iopub.status.idle": "2025-05-01T08:16:08.068619Z",
     "shell.execute_reply": "2025-05-01T08:16:08.067575Z"
    }
   },
   "outputs": [],
   "source": [
    "from brainbox import BrainBox\n",
    "from yo_fluq import *\n",
    "\n",
    "api = BrainBox.Api('127.0.0.1:8090')\n",
    "for file in Query.folder('../files/upsampling_example/wavs/'):\n",
    "    api.upload(file.name, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91320b54-282b-40ae-961a-0cd19f313ff9",
   "metadata": {},
   "source": [
    "First, you need to setup the Exporter. This entity will transform the `UpsamplingItem` into a much smaller `ExportItem`. In theory, for new voice cloners and new recognitions, we may want to design other UpsamplingItems with different set of fields, and Exporter will help to standardize them for annotation and dataset building. \n",
    "\n",
    "The conversion comes in two steps: first, the ExporterItem is created. Then, when needed, the wav-content is created, as we don't want to have it in memory all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a3803b1-c5b1-46bb-bcbb-df3854f287ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T08:16:08.070974Z",
     "iopub.status.busy": "2025-05-01T08:16:08.070663Z",
     "iopub.status.idle": "2025-05-01T08:16:08.881612Z",
     "shell.execute_reply": "2025-05-01T08:16:08.881280Z"
    }
   },
   "outputs": [],
   "source": [
    "from chara.voice_clone.training import TrimExporter, StringDistance\n",
    "\n",
    "exporter = TrimExporter(\n",
    "    api = api,\n",
    "    distance = StringDistance(),\n",
    "    trim_text_start = 'The beginning. ',\n",
    "    trim_text_end = ' The end.',\n",
    "    max_vosk_trim=6,\n",
    "    time_margin_in_seconds=0.2,\n",
    ")\n",
    "\n",
    "records = [exporter.export(rec) for rec in FileIO.read_pickle('../files/upsampling_example/records.pkl')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ef13dd-2d68-4eac-80fe-68973a1e8d48",
   "metadata": {},
   "source": [
    "This configuration will cut `trim_text_start` from the start of the text and `trim_text_end` from the end. It will also cut at most `max_vosk_trim` from the start and the end of the Vosk transcribtion so that the result matches the trimmed text the best in terms of `StringDistance`. This step is done by selecting the best of all the possible Vosk's trimmings, because there might be words added or removed to Vosk transcription.\n",
    "\n",
    "When exporting, the sound will be cut at `time_margin_in_seconds` before/after the first/last relevant word recognized by vosk. \n",
    "\n",
    "There is also `max_interword_margin` parameter that will remove the too-long pauses inside the sound fragment, if they are present.\n",
    "\n",
    "Let's now view the resulting array of the records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f127612-16e9-4360-8596-fa2a78bbefcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T08:16:08.882879Z",
     "iopub.status.busy": "2025-05-01T08:16:08.882801Z",
     "iopub.status.idle": "2025-05-01T08:16:08.887014Z",
     "shell.execute_reply": "2025-05-01T08:16:08.886858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "character\n",
       "Axe     11.97\n",
       "Lina    10.86\n",
       "Name: duration, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([r.__dict__ for r in records])\n",
    "df.groupby('character').duration.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0cc85-a224-41e2-9f4f-a6cf237c037b",
   "metadata": {},
   "source": [
    "Now we can annotate these records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a5526b-5e46-482e-b27c-cfb927623953",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T08:16:08.887940Z",
     "iopub.status.busy": "2025-05-01T08:16:08.887810Z",
     "iopub.status.idle": "2025-05-01T08:16:09.010329Z",
     "shell.execute_reply": "2025-05-01T08:16:09.010046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from chara.voice_clone.training import Annotator\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "path = Path('temp/annotation.txt')\n",
    "if path.is_file():\n",
    "    os.unlink(path)\n",
    "\n",
    "annotator = Annotator('temp/annotation.txt', Annotator.Data(records), exporter, (Annotator.Feedback.yes, \"No\"), randomize=False)\n",
    "_, share_url, __ = annotator.create_interface().launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef9d62-2435-4ae1-bd73-a3d467e10f8d",
   "metadata": {},
   "source": [
    "In the interface, you may choose the voice and then push the buttons to annotate the samples. Some buttons have special meaning to the algorithm, and must be created with the labels, defined in `Annotator.Feedback`. \n",
    "\n",
    "The following code is testing the created interface for the integration tests of the demo. You should remove it when working with this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9931ea94-1d15-441d-81eb-aae6127bb4ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T08:16:09.011390Z",
     "iopub.status.busy": "2025-05-01T08:16:09.011304Z",
     "iopub.status.idle": "2025-05-01T08:16:09.622934Z",
     "shell.execute_reply": "2025-05-01T08:16:09.621539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: http://127.0.0.1:7860/ âœ”\n"
     ]
    }
   ],
   "source": [
    "from gradio_client import Client\n",
    "client = Client(share_url)\n",
    "client.predict(voice=\"Lina\", api_name=\"/cm_voice_change\")\n",
    "client.predict(api_name=\"/cm_next_sample\")[3]['label']\n",
    "id1 = annotator._current_id\n",
    "client.predict('Yes',api_name=\"/cm_feedback\")\n",
    "client.predict(api_name=\"/cm_next_sample\")[3]['label']\n",
    "id2 = annotator._current_id\n",
    "client.predict('No', api_name='/cm_feedback')\n",
    "\n",
    "annotation_data = Annotator.read_annotation_file('temp/annotation.txt')\n",
    "annotation_data = {rec['id']:rec['feedback'] for rec in annotation_data}\n",
    "assert annotation_data[id1] == 'Yes'\n",
    "assert annotation_data[id2] == 'No'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7253b40c-9899-482d-b568-a805569249be",
   "metadata": {},
   "source": [
    "You may now use annotation data to filter the records. After (or instead of) that, you may export the records in the format that is supported by `PiperTraining`.\n",
    "\n",
    "Note: `PiperTraining` does support the multiple-voice models. However, my experience with them is that the quality is poorer than for several independent models. Thus, I would rather suggest creating a model per character, and for thet you need to create a dataset per each character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d004d73-c00b-4e04-96c3-b2d539e5500c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T08:16:09.627834Z",
     "iopub.status.busy": "2025-05-01T08:16:09.627361Z",
     "iopub.status.idle": "2025-05-01T08:16:09.638962Z",
     "shell.execute_reply": "2025-05-01T08:16:09.638095Z"
    }
   },
   "outputs": [],
   "source": [
    "characters = set(rec.character for rec in records)\n",
    "for character in characters:\n",
    "    exporter.export_to_zip([rec for rec in records if rec.character==character], f'temp/dataset_{character}.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8dfe4d0-06ac-4e2e-aea5-790d6abeff4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T08:16:09.642854Z",
     "iopub.status.busy": "2025-05-01T08:16:09.642429Z",
     "iopub.status.idle": "2025-05-01T08:16:09.647466Z",
     "shell.execute_reply": "2025-05-01T08:16:09.646578Z"
    }
   },
   "outputs": [],
   "source": [
    "assert Path('temp/dataset_Axe.zip').is_file()\n",
    "assert Path('temp/dataset_Lina.zip').is_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d98f7d3-5907-4c37-88f5-bca5b19c8a00",
   "metadata": {},
   "source": [
    "After this, the following steps are required:\n",
    "\n",
    "**Generate enough data**\n",
    "\n",
    "What is \"enough\" is debatable. With Tortoise and manual annotation I was able to train a good voice on as mush as 27 minutes. But in this case, the samples were hand-picked, that took a lot of time to annotate, and I wouldn't want to repeat this experience. \n",
    "\n",
    "For Zonos I don't need to annotate, but that means some failures make it to the dataset even after all the procedures with Vosk (e.g. coughing in the middle of the sentence). The quality is lower, and thus more data is required for training. 40 minutes were not enough, 60 minutes were enough with minor glitches, 90 minutes was okay (for German). Otherwise, there is a noise and \"sand\" in the records, probably coming from the rare cases when Zonos failed to produce voice properly. Bigger dataset definitely helps with this problem.\n",
    "\n",
    "**Run the training**\n",
    "\n",
    "Checkout the `PiperTraining` self-test on how to do it. You will need to set up the `TrainingSettings`, and here:\n",
    "\n",
    "* `language` should be set according to espeak notation\n",
    "* `base_model` you'll need to download, see `PiperTrainingSettings` for details. They are available here https://huggingface.co/datasets/rhasspy/piper-checkpoints/\n",
    "* `batch_size` should be small enough so you wouldn't run out of memory\n",
    "* `max_epochs` should be the a sum of how long the original model was training (available in the model's filename) and an amount of epochs you want to train the model. The epoch count for 60 minutes of dataset was 1500, and the quality was oscillating in the end. \n",
    "* If `keep_intermediate` is set to True, the intermediate checkpoints, not only the final one, will be kept and you will be able to pick the best one. I recommend turning this option on, but also adjust `checkpoint_epochs` to keep the disk from overfilling.\n",
    "\n",
    "The PiperTraining default usage creates a pair of training and exporting task, and if left alone, they will fully perform the prodecure. If you want to cancel the training or train for more steps, you can always do it by creating the training task manually. After such a sequence of trainings, manually created export task will convert the `ckpt` files to `onnx`, required for Piper.\n",
    "\n",
    "**Pick the best model**\n",
    "\n",
    "In my observations, it wasn't the case that the longer model trains the better result is. At the late stages, the quality oscillates, probably because the model walking in cycles. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
